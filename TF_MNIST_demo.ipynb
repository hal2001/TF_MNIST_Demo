{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import some python libraries.  The most important of course is the TensorFlow library itself.  We also need the os library for doing some file path computations.  Finally, the urllib library allows us to load data stored somewhere on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the new Tensorflow 1.0 API from Google to classify the MNIST handwritten digits dataset.\n",
    "More information about TensorFlow can be found at https://www.tensorflow.org/.  This demo is heavily based on material presented by Dandelion Mané (of Google) at the TensorFlow Dev Summit 2017 in his excellent talk: Hands-on TensorBoard (https://www.youtube.com/watch?v=eBbEDRsCmv4&index=4&list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv) ....very worth watching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# We need to set up some file locations etc... #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we need to define a logging directory to be used in the experiments that follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = 'c:/Users/lau/tmp/Demo1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dandelion Mané has very kindly set up a GitHub gist (think of a gist as a simplified Github repo, it is just used for sharing small pieces of code and examples) with everything we need in terms of data, here is the path to this gist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GIST_URL = 'https://gist.githubusercontent.com/dandelionmane/4f02ab8f1451e276fea1f165a20336f1/raw/a20c87f5e1f176e9abf677b46a74c6f2581c7bd8/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the so called MNIST data (more info at http://yann.lecun.com/exdb/mnist/).  Notice that TensorFlow provides a function for loading these data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting c:/Users/lau/tmp/Demo1/data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting c:/Users/lau/tmp/Demo1/data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting c:/Users/lau/tmp/Demo1/data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting c:/Users/lau/tmp/Demo1/data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir = LOGDIR + 'data', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dandelion Mané has kindly provided us with some data that we need to make the embedding demo work (more on this later...).  Basically we are loading a sprite and labels file for the embedding projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:/Users/lau/tmp/Demo1/sprite_1024.png',\n",
       " <http.client.HTTPMessage at 0x9316278>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(GIST_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urllib.request.urlretrieve(GIST_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to define a few convenience functions #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first function basically makes it simple to compose some information strings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hparam_string(learning_rate, num_convs, num_fully_connected):\n",
    "    return 'LR {0} Conv layers {1} Fully connected layers {2}'.format(learning_rate, num_convs, num_fully_connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function uses TensorFlow syntax to define a python function that sets up a convolutional layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, size_in, size_out, conv_size = 3, conv_stride = 1, pool_factor = 1, pool_stride = 1, name = \"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        # Initialize weights with a truncated normal distribution\n",
    "        w = tf.Variable(tf.truncated_normal([conv_size, conv_size, size_in, size_out], stddev = 0.1), name = \"W\")\n",
    "        # Set the biases to be constants\n",
    "        b = tf.Variable(tf.constant(0.1, shape = [size_out]), name = \"B\")\n",
    "        # Perform the actual convolution\n",
    "        conv = tf.nn.conv2d(input, w, strides = [1, conv_stride, conv_stride, 1], padding = \"SAME\")\n",
    "        # Apply a rectified linear unit to the convolution result\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        # Dump information to the summary process\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        # Apply maxpooling and return\n",
    "        return tf.nn.max_pool(act, ksize = [1, pool_factor, pool_factor, 1], strides = [1, pool_stride, pool_stride, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function defines a python function that sets up a fully connected layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, size_in, size_out, name = \"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        # Initialize weights with a truncated normal distribution\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev = 0.1), name = \"W\")\n",
    "        # Set the biases to be constants\n",
    "        b = tf.Variable(tf.constant(0.1, shape = [size_out]), name = \"B\")\n",
    "        # Apply a rectified linear unit to the output\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        # Dump information to the summary process\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        # Return\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes a monster, it sets up the netowrk we want and trains it. It takes a learning rate and a descriptive string as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_model(learning_rate, hparam):\n",
    "\n",
    "    # Clear the default graph stack and reset the global default graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Set up a session\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Setup placeholders for the image data and reshape the data for display\n",
    "    x = tf.placeholder(tf.float32, shape = [None, 784], name = \"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    # Dump information to the summary process\n",
    "    tf.summary.image('input', x_image, 6)\n",
    "    \n",
    "    # Setup placeholders for the label data\n",
    "    y = tf.placeholder(tf.float32, shape = [None, 10], name = \"labels\")\n",
    "\n",
    "    # Define network with three convolutional layers and two fully connected layer\n",
    "    conv1 =    conv_layer(x_image, 1,  32, conv_size = 3, conv_stride = 1, pool_factor = 2, pool_stride = 2, name = \"conv1\")\n",
    "    conv2 =    conv_layer(conv1,  32,  64, conv_size = 3, conv_stride = 1, pool_factor = 2, pool_stride = 2, name = \"conv2\")\n",
    "\n",
    "    # Flatten to prepare for the fully connected layers\n",
    "    #flattened = tf.reshape(conv_out, [-1, 28 * 28 * 128])\n",
    "    flattened = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Define fully connected layers\n",
    "    #fc1 = fc_layer(flattened, 28 * 28 * 128, 1024, \"fc1\")\n",
    "    fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "    embedding_input = fc1\n",
    "    embedding_size = 1024\n",
    "    logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "\n",
    "    # Calculate the cross entropy and send to summary writer\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y), name = \"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    # Then train\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    # Put all summaries into one chunk\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "    # Then prepare for showing the embedding\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name = \"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    \n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    # Now train for 2000 iterations\n",
    "    for i in range(20001):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict = {x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "        if i % 500 == 0:\n",
    "            #sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally run it all and enjoy #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run for LR 5e-05 Conv layers 2 Fully connected layers 1\n"
     ]
    }
   ],
   "source": [
    "# Define a learning rate\n",
    "learning_rate = 5E-5\n",
    "\n",
    "# Construct a hyperparameter string to describe what we are up to\n",
    "hparam = make_hparam_string(learning_rate, 2, 1)\n",
    "print('Starting run for %s' % hparam)\n",
    "\n",
    "# Actually run with the new settings\n",
    "mnist_model(learning_rate, hparam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
